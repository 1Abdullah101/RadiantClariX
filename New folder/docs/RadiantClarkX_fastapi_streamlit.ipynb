{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166553e6-d135-478a-b036-0078100104a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REACT-NATIVE (FAST API) VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81738a62-f4b1-430a-9754-963a0269685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unccomment the following, to install required packages if running for the first time.\n",
    "#!pip install fastapi python-multipart uvicorn nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caba80e7-0cfa-45e1-89e9-1ced309a81a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing radiant_api.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile radiant_api.py\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from transformers import BlipForConditionalGeneration, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import io\n",
    "import uvicorn\n",
    "from pathlib import Path\n",
    "\n",
    "# ======== CONFIG - update if your path differs ========\n",
    "EXPORT_PATH = Path(r\"F:\\Dynoid Development\\RadiantClarkiX\\Chest Model\\model_export\")\n",
    "MODEL_DIR = EXPORT_PATH / \"hf_model\"\n",
    "PROCESSOR_DIR = EXPORT_PATH / \"hf_processor\"\n",
    "# =====================================================\n",
    "\n",
    "app = FastAPI(title=\"RadiantClarkX API\")\n",
    "\n",
    "# Allow cross-origin requests for development (Expo Snack / Expo Go)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device} (if CUDA available, will use GPU)\")\n",
    "\n",
    "# Load model and processor once (cache)\n",
    "# If you get a warning about \"use_fast\" you can safely ignore it for now\n",
    "processor = AutoProcessor.from_pretrained(str(PROCESSOR_DIR))\n",
    "model = BlipForConditionalGeneration.from_pretrained(str(MODEL_DIR)).to(device)\n",
    "model.eval()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict_image(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    POST /predict\n",
    "    Form-data: file -> image (jpg/png)\n",
    "    Response: {\"caption\": \"...\" }\n",
    "    \"\"\"\n",
    "    image_bytes = await file.read()\n",
    "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "    # Prepare inputs and generate\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_length=128, num_beams=5)\n",
    "        caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return {\"caption\": caption}\n",
    "\n",
    "# ONLY use uvicorn.run if you execute this file directly from terminal.\n",
    "# If running in Jupyter, see instructions below (use nest_asyncio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f5b6c-d5cd-4511-8e45-2cb6655d7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STREAMLIT VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5435ed9a-5907-4875-b6a4-9880c1b04fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting radiant.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile radiant.py\n",
    "import streamlit as st\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import BlipForConditionalGeneration, AutoProcessor, LogitsProcessorList\n",
    "\n",
    "# --- Config ---\n",
    "st.set_page_config(\n",
    "    page_title=\"Chest Image Captioning Demo\",\n",
    "    page_icon=\"ðŸ©º\",\n",
    "    layout=\"centered\",\n",
    "    initial_sidebar_state=\"auto\",\n",
    ")\n",
    "\n",
    "# Sidebar\n",
    "st.sidebar.title(\"ðŸ“‹ Instructions\")\n",
    "st.sidebar.write(\"\"\"\n",
    "1. Upload a chest image (X-ray or photo).  \n",
    "2. The model will generate a caption.  \n",
    "3. Use for demo purpose only â€” not medical advice.\n",
    "\"\"\")\n",
    "\n",
    "# Load model & processor\n",
    "EXPORT_PATH = Path(r\"F:\\\\Dynoid Development\\\\RadiantClarkiX\\\\Chest Model\\\\model_export\")\n",
    "MODEL_DIR = EXPORT_PATH / \"hf_model\"\n",
    "PROCESSOR_DIR = EXPORT_PATH / \"hf_processor\"\n",
    "\n",
    "@st.cache_resource  # caches loading so itâ€™s fast on rerun\n",
    "def load_model():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = BlipForConditionalGeneration.from_pretrained(str(MODEL_DIR)).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(str(PROCESSOR_DIR))\n",
    "    return model, processor, device\n",
    "\n",
    "model, processor, device = load_model()\n",
    "\n",
    "# Title\n",
    "st.title(\"ðŸ©» Chest Image Captioning with BLIP\")\n",
    "\n",
    "# File uploader\n",
    "uploaded_file = st.file_uploader(\"Upload a chest image\", type=[\"jpg\",\"jpeg\",\"png\"], accept_multiple_files=False)\n",
    "if uploaded_file:\n",
    "    image = Image.open(uploaded_file).convert(\"RGB\")\n",
    "    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "\n",
    "    # Run inference\n",
    "    with st.spinner(\"Generating captionâ€¦\"):\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "\n",
    "        # Decode caption\n",
    "        caption = processor.batch_decode(output.sequences, skip_special_tokens=True)[0]\n",
    "\n",
    "        # Compute confidence (mean of softmax probabilities across generated tokens)\n",
    "        scores = output.scores  # list of logits for each step\n",
    "        probs = [torch.nn.functional.softmax(s, dim=-1) for s in scores]\n",
    "        token_probs = []\n",
    "        for i, token_id in enumerate(output.sequences[0][1:]):  # skip <bos>\n",
    "            token_prob = probs[i][0, token_id].item()\n",
    "            token_probs.append(token_prob)\n",
    "\n",
    "        confidence = sum(token_probs) / len(token_probs) if token_probs else 0\n",
    "        confidence_pct = round(confidence * 100, 2)\n",
    "\n",
    "    # Display result\n",
    "    st.markdown(\n",
    "    f\"\"\"\n",
    "    <h3 style='font-size: 24px; color: #2E86C1; font-weight: 700;'>\n",
    "        ðŸ©º Caption: <span style='color: white;'>{caption}</span><br>\n",
    "        ðŸ”¹ Confidence: <span style='color: #00FF00;'>{confidence_pct}%</span>\n",
    "    </h3>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "    # Badge to show done\n",
    "    st.success(\"âœ… Caption generated successfully!\")\n",
    "\n",
    "# Optional extra UI\n",
    "st.markdown(\"---\")\n",
    "st.write(\"Demo built with Streamlit version:\", st.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
